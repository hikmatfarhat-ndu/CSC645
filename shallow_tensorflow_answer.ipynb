{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "shallow-tensorflow-answer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/shallow_tensorflow_answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1GbGYpm8pb4",
        "colab_type": "text"
      },
      "source": [
        "## Using Tensorflow to model the shallow network\n",
        "In this exercise will we redo, using tensorflow the shallow network that we trained from first\n",
        "principles before to recognize the \"flower\" shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc6230u28pb8",
        "colab_type": "text"
      },
      "source": [
        "### Reading the data\n",
        "First recall that tensorflow stacks the samples row-wise instead of column-wise\n",
        "as we have been doing when we did the gradient descent oursleves. Therefore in the last line of the\n",
        "function load_dataset() below we don't take the transpose of X and Y as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0ObDaVD8pb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset(n):\n",
        "    np.random.seed(1)\n",
        "    m = n # number of examples\n",
        "    N = int(m/2) # number of points per class\n",
        "    D = 2 # dimensionality\n",
        "    X = np.zeros((m,D),dtype='float32') # data matrix where each row is a single example\n",
        "    Y = np.zeros((m,1), dtype='float32') # labels vector (0 for red, 1 for blue)\n",
        "    a = 4 # maximum ray of the flower\n",
        "\n",
        "    for j in range(2):\n",
        "        ix = range(N*j,N*(j+1))\n",
        "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
        "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
        "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "        Y[ix] = j\n",
        "\n",
        "    return X, Y\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZL7owU98pcP",
        "colab_type": "text"
      },
      "source": [
        "### Defining the parameters\n",
        "Below we define the parameters that are needed. We know that n_x=2 and n_y=1 but we extract them from the shape of\n",
        "X_data and Y_data after we call load_dataset() . We also set the number\n",
        "of data points to 500."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZl1BVG48pcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 5\n",
        "nb_iterations = 10000\n",
        "num_data=500 #number of data points\n",
        "X,Y=load_dataset(num_data)\n",
        "\n",
        "#X_data,Y_data=load_dataset(num_data)# load data\n",
        "# Network Parameters\n",
        "n_h = 4 # number of neurons in hidden layer\n",
        "n_x = X.shape[1] #number of neurons in input\n",
        "n_y = Y.shape[1] #number of neurons in ouput\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZUQKDd8pce",
        "colab_type": "text"
      },
      "source": [
        "### Initialization\n",
        "\n",
        "Since tensorflow stacks the data row-wise the forward propagation is slightly different then we are used to.\n",
        "Let $W^1$,$W^2$,$b^1$,$b^2$ be the weights and biases of the first and second layer respectively then forward propagation is define as\n",
        "\\begin{align*}\n",
        "Z^1&=X\\cdot W^1+b^1\\\\\n",
        "A^1 &=\\sigma(Z^1)\\\\\n",
        "Z^2 &=A^1\\cdot W^2+b2\\\\\n",
        "A^2 &=\\sigma(Z^2)\n",
        "\\end{align*}\n",
        "Accorging to the above equations you have to define the tensorflow variables that will hold the weights and biases. \n",
        "The biases are set to zero using the tensorflow function tf.zeros([size]) and the weights randomly using tf.random_normal([size1,size2]) using the appropriate sizes.\n",
        "Also we have to define two placeholders for the data X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY46Q5c38pch",
        "colab_type": "code",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d418031-b813-4495-8846-6f6342bacbc1"
      },
      "source": [
        "\n",
        "initializer = tf.initializers.RandomNormal()\n",
        "\n",
        "#W1=tf.Variable(tf.random.normal([n_x,n_h]),dtype='float32')#Weights of the first layer\n",
        "W1=tf.Variable(initializer([n_x,n_h]),trainable=True,dtype=tf.float32)\n",
        "W2=tf.Variable(initializer([n_h,n_y]),trainable=True,dtype=tf.float32)\n",
        "\n",
        "#W2=tf.Variable(tf.random.normal([n_h,n_y]),dtype='float32')#weights of the second layer\n",
        "b1=tf.Variable(tf.zeros([n_h]))            #biases of the first layer\n",
        "b2=tf.Variable(tf.zeros([n_y]))            #biases of the second layer\n",
        "print(n_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0PPEanu8pct",
        "colab_type": "text"
      },
      "source": [
        "### Defining the model\n",
        "Our model has two layers. The function \"model\" below should return the ouput of our model for a given input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHUGz6ki8pcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(input):\n",
        "    # Hidden fully connected layer with 256 neurons\n",
        "   \n",
        "    layer_1 = tf.add(tf.matmul(input, W1), b1)\n",
        "    # Output fully connected layer with a neuron for each class\n",
        "    out_layer = tf.matmul(tf.sigmoid(layer_1), W2) + b2\n",
        "    return out_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYvEYQzY8pc7",
        "colab_type": "text"
      },
      "source": [
        "Once the model is defined the remaining code is similar to our previous exercise. We define the loss\n",
        "as an average over the cross-entropy but this time since it is binary classification we use the sigmoid instead\n",
        "of the softmax function. Then our optimizer uses gradient descent to minimize the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyOu53Aa8pdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Define loss and optimize\n",
        "def loss(pred,label):\n",
        "   return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=label))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyKqGz848pdQ",
        "colab_type": "text"
      },
      "source": [
        "The model is defined now we run our computation in a session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3bNNi0M8pdS",
        "colab_type": "code",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "a9735e52-56c6-4173-d413-f60a6f2b3712"
      },
      "source": [
        "# Initializing the variables\n",
        "optimizer=tf.optimizers.SGD(learning_rate)\n",
        "def train(model,input,output):\n",
        "  with tf.GradientTape() as tape:\n",
        "    diff=loss(model(X),Y)\n",
        "  grad=tape.gradient(diff,[W1,W2,b1,b2])\n",
        "  optimizer.apply_gradients( zip( grad , [W1,W2,b1,b2] ) )\n",
        "print(loss(model(X),Y))\n",
        "\n",
        "for i in range(nb_iterations):\n",
        " if(i%100==0):\n",
        "   print(loss(model(X),Y))\n",
        " train(model,X,Y)\n",
        " \n",
        "def prediction(X):\n",
        "  a=tf.math.sigmoid(model(X))\n",
        "  return tf.cast((a>0.5),tf.int32)\n",
        "pT=tf.transpose(prediction(X))\n",
        "print(np.dot(pT,Y))\n",
        "print(np.dot(1-pT,1-Y))\n",
        "correct=np.dot(pT,Y)+np.dot(1-pT,1-Y)\n",
        "accuracy=100*float(np.squeeze(correct))/float(Y.shape[0])\n",
        "print(\"Accuracy=\"+str(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjTFhXd8pdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}