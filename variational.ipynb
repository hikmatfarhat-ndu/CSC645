{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "variational.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyONSqp1JJNwZhI7mRcmKPTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/variational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHovWB6Nps2a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx86MS1Op49f"
      },
      "source": [
        "\n",
        "'''Example of VAE on MNIST dataset using MLP\n",
        "\n",
        "The VAE has a modular design. The encoder, decoder and VAE\n",
        "are 3 models that share weights. After training the VAE model,\n",
        "the encoder can be used to generate latent vectors.\n",
        "The decoder can be used to generate MNIST digits by sampling the\n",
        "latent vector from a Gaussian distribution with mean = 0 and std = 1.\n",
        "\n",
        "# Reference\n",
        "\n",
        "[1] Kingma, Diederik P., and Max Welling.\n",
        "\"Auto-Encoding Variational Bayes.\"\n",
        "https://arxiv.org/abs/1312.6114\n",
        "'''\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.layers import Lambda, Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import mse, binary_crossentropy\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os    "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXi3XvLu0Wdm"
      },
      "source": [
        "# reparameterization trick\n",
        "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
        "# z = z_mean + sqrt(var) * epsilon\n",
        "def sampling(args):\n",
        "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
        "\n",
        "    # Arguments\n",
        "        args (tensor): mean and log of variance of Q(z|X)\n",
        "\n",
        "    # Returns\n",
        "        z (tensor): sampled latent vector\n",
        "    \"\"\"\n",
        "\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    # by default, random_normal has mean = 0 and std = 1.0\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txEISiUg0Jwt"
      },
      "source": [
        "def plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=128,\n",
        "                 model_name=\"vae_mnist\"):\n",
        "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
        "\n",
        "    # Arguments\n",
        "        models (tuple): encoder and decoder models\n",
        "        data (tuple): test data and label\n",
        "        batch_size (int): prediction batch size\n",
        "        model_name (string): which model is using this function\n",
        "    \"\"\"\n",
        "\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    os.makedirs(model_name, exist_ok=True)\n",
        "\n",
        "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = encoder.predict(x_test,\n",
        "                                   batch_size=batch_size)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
        "    # display a 30x30 2D manifold of digits\n",
        "    n = 30\n",
        "    digit_size = 28\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-4, 4, n)\n",
        "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = (n - 1) * digit_size + start_range + 1\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap='Greys_r')\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01qfi7mM0C8k",
        "outputId": "15bcecfe-2ff3-4a8e-ec41-0de50fe210fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "original_dim = image_size * image_size\n",
        "x_train = np.reshape(x_train, [-1, original_dim])\n",
        "x_test = np.reshape(x_test, [-1, original_dim])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj8pL6Rqz-pa"
      },
      "source": [
        "# network parameters\n",
        "input_shape = (original_dim, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 128\n",
        "latent_dim = 2\n",
        "epochs = 50\n",
        "\n",
        "# VAE model = encoder + decoder\n",
        "# build encoder model\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# use reparameterization trick to push the sampling out as input\n",
        "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPiNfZE-z5gD",
        "outputId": "c0583667-fa86-4e45-e6bc-f5338af4dd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "# instantiate encoder model\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# build decoder model\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
        "\n",
        "# instantiate decoder model\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# instantiate VAE model\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n",
        "\n",
        "\n",
        "models = (encoder, decoder)\n",
        "data = (x_test, y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 784)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          401920      encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 2)            1026        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 2)            1026        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 403,972\n",
            "Trainable params: 403,972\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_sampling (InputLayer)      [(None, 2)]               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               1536      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 784)               402192    \n",
            "=================================================================\n",
            "Total params: 403,728\n",
            "Trainable params: 403,728\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pz7KCgtz2da"
      },
      "source": [
        "# VAE loss = mse_loss or xent_loss + kl_loss\n",
        "\n",
        "#reconstruction_loss = mse(inputs, outputs)\n",
        "reconstruction_loss = binary_crossentropy(inputs,\n",
        "                                                  outputs)\n",
        "\n",
        "reconstruction_loss *= original_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4kjCeDJzudb",
        "outputId": "9494d9af-ad7e-44f9-9a59-0703fb7b974d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "vae.fit(x_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test, None))\n",
        "vae.save_weights('vae_mlp_mnist.h5')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 200.0667 - val_loss: 174.6147\n",
            "Epoch 2/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 171.3011 - val_loss: 168.3233\n",
            "Epoch 3/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 167.2563 - val_loss: 165.6053\n",
            "Epoch 4/50\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 164.8126 - val_loss: 163.6101\n",
            "Epoch 5/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 162.9150 - val_loss: 162.0718\n",
            "Epoch 6/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 161.5007 - val_loss: 161.0540\n",
            "Epoch 7/50\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 160.3536 - val_loss: 159.8897\n",
            "Epoch 8/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 159.3600 - val_loss: 158.9463\n",
            "Epoch 9/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 158.4348 - val_loss: 158.3992\n",
            "Epoch 10/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 157.5381 - val_loss: 157.6001\n",
            "Epoch 11/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 156.7595 - val_loss: 156.8731\n",
            "Epoch 12/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 156.0315 - val_loss: 156.1032\n",
            "Epoch 13/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 155.3727 - val_loss: 155.9871\n",
            "Epoch 14/50\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 154.8662 - val_loss: 155.2390\n",
            "Epoch 15/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 154.3753 - val_loss: 155.1313\n",
            "Epoch 16/50\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 153.9770 - val_loss: 154.9017\n",
            "Epoch 17/50\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 153.4975 - val_loss: 154.2710\n",
            "Epoch 18/50\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 153.2031 - val_loss: 154.3527\n",
            "Epoch 19/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 152.8146 - val_loss: 154.3028\n",
            "Epoch 20/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 152.5145 - val_loss: 154.0089\n",
            "Epoch 21/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 152.2262 - val_loss: 153.7296\n",
            "Epoch 22/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 151.8498 - val_loss: 153.8232\n",
            "Epoch 23/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 151.6068 - val_loss: 153.4713\n",
            "Epoch 24/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 151.4135 - val_loss: 153.6808\n",
            "Epoch 25/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 151.1753 - val_loss: 153.6439\n",
            "Epoch 26/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 150.8812 - val_loss: 153.2263\n",
            "Epoch 27/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 150.6825 - val_loss: 153.9112\n",
            "Epoch 28/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 150.4869 - val_loss: 153.2985\n",
            "Epoch 29/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 150.2708 - val_loss: 152.9728\n",
            "Epoch 30/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 150.1115 - val_loss: 152.7158\n",
            "Epoch 31/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 149.9331 - val_loss: 153.2033\n",
            "Epoch 32/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 149.7276 - val_loss: 152.8647\n",
            "Epoch 33/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 149.5877 - val_loss: 152.4243\n",
            "Epoch 34/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 149.3609 - val_loss: 152.4607\n",
            "Epoch 35/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 149.2061 - val_loss: 152.6544\n",
            "Epoch 36/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 149.0472 - val_loss: 152.7710\n",
            "Epoch 37/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 148.9169 - val_loss: 152.2480\n",
            "Epoch 38/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 148.7180 - val_loss: 152.2104\n",
            "Epoch 39/50\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 148.5683 - val_loss: 152.2593\n",
            "Epoch 40/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 148.4537 - val_loss: 152.3591\n",
            "Epoch 41/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 148.2716 - val_loss: 151.9268\n",
            "Epoch 42/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 148.1618 - val_loss: 152.2664\n",
            "Epoch 43/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 148.0752 - val_loss: 151.8775\n",
            "Epoch 44/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.8885 - val_loss: 152.1201\n",
            "Epoch 45/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.7296 - val_loss: 151.7018\n",
            "Epoch 46/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.6248 - val_loss: 151.7285\n",
            "Epoch 47/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.4935 - val_loss: 152.1818\n",
            "Epoch 48/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.4168 - val_loss: 152.2221\n",
            "Epoch 49/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.3214 - val_loss: 151.5741\n",
            "Epoch 50/50\n",
            "469/469 [==============================] - 10s 22ms/step - loss: 147.1854 - val_loss: 151.9181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5SWtuccp-OV"
      },
      "source": [
        "plot_results(models,\n",
        "                 data,\n",
        "                 batch_size=batch_size,\n",
        "                 model_name=\"vae_mlp\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}