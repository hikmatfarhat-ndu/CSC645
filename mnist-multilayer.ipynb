{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "28bd451f-2b86-42dc-97f3-79553a06e4b8",
   "display_name": "'Python Interactive'"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/mnist-multilayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# A deep feedforward network with arbitrary depth\n",
    "\n",
    "In this exercise we build a deep feedforward neural network to recognize handwrittne digits (the MNIST data). The code has many tweekable parameters, in particular the number and width of layers. But as you will see adding more layers slows down the convergence considerably.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The usual imports plus gzip and pickle to read the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gzip\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "Important to be able to diplay matplotlib plots inline in the notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%matplotlib inline"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "The data are in compressed pickle format. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with gzip.open(\"mnist.pkl.gz\") as f:\n",
    "        data=pickle._Unpickler(f)\n",
    "        data.encoding='latin1'\n",
    "        tr,_,te=data.load()\n",
    "    X=tr[0]\n",
    "    Y=tr[1]\n",
    "    X=X.T\n",
    "    Y=Y.reshape(1,len(Y))\n",
    "    V=np.zeros((10,Y.shape[1]))\n",
    "    for j in range(Y.shape[1]):\n",
    "        V[Y[0,j],j]=1\n",
    "        \n",
    "    Y=V\n",
    "    test_data=te[0]\n",
    "    test_labels=te[1]\n",
    "    test_data=test_data.T\n",
    "    test_labels=test_labels.reshape(1,len(test_labels))\n",
    "    return X,Y,test_data,test_labels\n"
   ]
  },
  {
   "source": [
    "The activation function is sigmoid"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "source": [
    "This function plots some of the misclassified data in order for us to have an idea what when wrong."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_misclassified(test_data,test_labels,biases,weights):\n",
    "    As=forward_propagation(test_data,biases,weights)\n",
    "    output=As[-1]\n",
    "    count=0\n",
    "    fig=plt.figure()\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust( wspace=1, hspace=1)\n",
    "\n",
    "    for i in range(output.shape[1]):\n",
    "        label=np.argmax(output[:,i])\n",
    "        if label != test_labels[0,i]:\n",
    "            if count>40:\n",
    "                break\n",
    "            subfig=count%40+1\n",
    "            count=count+1\n",
    "            img=test_data[:,i].reshape(28,28)\n",
    "            t=fig.add_subplot(4,10,subfig)\n",
    "            #t.set_title(str(i))\n",
    "            t.set_title(str(label))\n",
    "            t.axes.get_xaxis().set_visible(False)\n",
    "            t.axes.get_yaxis().set_visible(False)\n",
    "            plt.imshow(img,cmap='gray_r')\n",
    "\n"
   ]
  },
  {
   "source": [
    "Returns the number of correct predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_data,biases,weights):\n",
    "    As=forward_propagation(test_data,biases,weights)\n",
    "    output=As[-1]\n",
    "    count=0\n",
    "    for i in range(output.shape[1]):\n",
    "        if np.argmax(output[:,i])==test_labels[0,i]:\n",
    "            count=count+1\n",
    "    return count"
   ]
  },
  {
   "source": [
    "The usual cross Entropy cost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y,b,w):\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    As=forward_propagation(X,b,w)\n",
    "    # recall that As contains the \"output\" of all layers\n",
    "    # including the input As[0] and the final output As[-1]\n",
    "    output=As[-1]\n",
    "    logprobs = np.log(output)*Y+np.log(1-output)*(1-Y)\n",
    "    cost = -np.sum(logprobs)/m\n",
    "\n",
    "    count=0\n",
    "    for i in range(output.shape[1]):\n",
    "        if (np.argmax(output[:,i])==np.argmax(Y[:,i])):\n",
    "            count=count+1\n",
    "\n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost,count\n"
   ]
  },
  {
   "source": [
    "The weights are initialized randomly and the biases are initially set to zero"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(width):    \n",
    "    weights=[]\n",
    "    biases=[]\n",
    "    numlayers=num_layers\n",
    "    for i in range(len(width)-1):\n",
    "        print(\"size {},{}\".format(width[i+1],width[i]))\n",
    "        w=np.random.randn(width[i+1],width[i])\n",
    "        b=np.zeros((width[i+1],1))\n",
    "        weights.append(w)\n",
    "        biases.append(b)\n",
    "\n",
    "    return biases,weights\n"
   ]
  },
  {
   "source": [
    "Forward propagation over all the layers but also retain the intermediate results.\n",
    "For example below As[0] is the input, As[1] is the output of the first layer,..., and As[-1] (the last) is the output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,biases,weights):\n",
    "    a=X\n",
    "    As=[X]\n",
    "    for w,b in zip(weights,biases):\n",
    "       z=np.dot(w,a)+b\n",
    "       a=sigmoid(z)\n",
    "       As.append(a)\n",
    "\n",
    "    return As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X,Y,biases,weights):\n",
    "\n",
    "    As=forward_propagation(X,biases,weights)\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    nlayers=len(biases)\n",
    "    dz=As[nlayers]-Y\n",
    "#   uncomment below if you want to use mean squared error\n",
    "#    dz=dz*As[nlayers]*(1-As[nlayers])\n",
    "    gradb=[]\n",
    "    gradw=[]\n",
    "    for i in range(nlayers,0,-1):\n",
    "        db=np.sum(dz,axis=1,keepdims=True)/m\n",
    "        dw=np.dot(dz,As[i-1].T)/m\n",
    "        dz=np.dot(weights[i-1].T,dz)*As[i-1]*(1-As[i-1])\n",
    "        gradb.insert(0,db)\n",
    "        gradw.insert(0,dw)\n",
    "    \n",
    "    return gradb,gradw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(biases,weights, gradb,gradw, learning_rate):\n",
    "\n",
    "\n",
    "    for i in range(len(biases)):\n",
    "        weights[i]=weights[i]-learning_rate*gradw[i]\n",
    "        biases[i]=biases[i]-learning_rate*gradb[i]\n",
    "\n",
    "    return biases,weights"
   ]
  },
  {
   "source": [
    "Initialize the parameters\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(X, Y, test_data,width,batch_size,num_iterations, learning_rate,print_cost=False):\n",
    "\n",
    "    #n_x=X.shape[0]\n",
    "    #n_y=Y.shape[0]\n",
    "    biases,weights=initialize_parameters(width)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        cost,count = compute_cost(Y,biases,weights)\n",
    "       \n",
    "        for k in range(0,X.shape[1],batch_size):\n",
    "            idx=[random.randint(0,Y.shape[1]-1) for s in range(batch_size)]\n",
    "            yb=Y[:,idx]\n",
    "            xb=X[:,idx]\n",
    "            gradb,gradw=backward_propagation(xb,yb,biases,weights)\n",
    "            biases,weights=update_parameters(biases,weights,gradb,gradw,learning_rate)    \n",
    "\n",
    "        if i%1 ==0 : \n",
    "            print(len(weights))   \n",
    "            count_test=evaluate(test_data,biases,weights)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            print(\"count after iteration %i is %i\" %(i,count))\n",
    "            print(\"test count after iteration %i is %i\" %(i,count_test))\n",
    "    return biases,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, Y,test_data,test_labels = load_dataset()\n",
    "n_x=X.shape[0]\n",
    "n_y=Y.shape[0]\n",
    "width=[n_x,256,128,64,n_y]\n",
    "biases,weights= GD(X, Y,test_data,width,batch_size=10,learning_rate=3,\n",
    "                num_iterations =10, print_cost=True)\n",
    "\n",
    "\n",
    "count=evaluate(test_data,biases,weights)\n",
    "A=forward_propagation(X,biases,weights)\n",
    "print(A[-1].shape)\n",
    "print(len(A))\n"
   ]
  },
  {
   "source": [
    "Prints some of the misclassified digits. The top on each digit shows the (wrong) prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_misclassified(test_data,test_labels,biases,weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}