{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "mnist-multilayer.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbDWs8GDdCPB"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/3mnist-multilayer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pRwpeuRdCPG"
      },
      "source": [
        "# A deep feedforward network with arbitrary depth\n",
        "\n",
        "In this exercise we build a deep feedforward neural network to recognize handwrittne digits (the MNIST data). The code has many tweekable parameters, in particular the number and width of layers. But as you will see adding more layers slows down the convergence considerably.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AsZFi1WdCPI"
      },
      "source": [
        "The usual imports plus gzip and pickle to read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IE6Iu3NdCPL"
      },
      "source": [
        "#import numpy as np\n",
        "import cupy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from tensorflow.keras import layers"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjGvLGMYdCPf"
      },
      "source": [
        "Important to be able to diplay matplotlib plots inline in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xZzopv8dCPh"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ysbGnUdJOE"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqvMrrX8dCPs"
      },
      "source": [
        "The data are in compressed pickle format. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3oAnz2VdCPu"
      },
      "source": [
        "def load_dataset():\n",
        "  # tensorflow assumes the input and output are row vectors\n",
        "  # where as in our implementation we use them as column vectors\n",
        "    tr,te=keras.datasets.mnist.load_data()\n",
        "    X=tr[0].astype(\"float32\")/255\n",
        "    Y=tr[1]\n",
        "    Y=Y.reshape(1,len(Y))\n",
        "    V=np.zeros((10,Y.shape[1]))\n",
        "    for j in range(Y.shape[1]):\n",
        "        V[Y[0,j],j]=1\n",
        "        \n",
        "    Y=V.astype(\"float32\")\n",
        "    test_data=te[0].astype(\"float32\")/255\n",
        "    test_labels=te[1].astype(\"float32\")\n",
        "    test_labels=test_labels.reshape(1,len(test_labels))\n",
        "    return X,Y.T,test_data,test_labels.T\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCCd-Mi1oxJb",
        "outputId": "d22344cb-661e-4733-a993-041825831594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X, Y,test_data,test_labels = load_dataset()\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "print(test_data.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 10)\n",
            "(10000, 28, 28)\n",
            "(10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdhVYpgYdCP7"
      },
      "source": [
        "The activation function is sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLYWfHridCP-"
      },
      "source": [
        "def sigmoid(x):\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQPz1c9bdCQJ"
      },
      "source": [
        "This function plots some of the misclassified data in order for us to have an idea what when wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1MkdQ8dCQL"
      },
      "source": [
        "def print_misclassified(test_data,test_labels,biases,weights):\n",
        "    As=forward_propagation(test_data,biases,weights)\n",
        "    output=As[-1]\n",
        "    count=0\n",
        "    fig=plt.figure()\n",
        "    fig.tight_layout()\n",
        "    plt.subplots_adjust( wspace=1, hspace=1)\n",
        "\n",
        "    for i in range(output.shape[1]):\n",
        "        label=np.argmax(output[:,i])\n",
        "        if label != test_labels[0,i]:\n",
        "            if count>40:\n",
        "                break\n",
        "            subfig=count%40+1\n",
        "            count=count+1\n",
        "            img=test_data[:,i].reshape(28,28)\n",
        "            t=fig.add_subplot(4,10,subfig)\n",
        "            #t.set_title(str(i))\n",
        "            t.set_title(str(label))\n",
        "            t.axes.get_xaxis().set_visible(False)\n",
        "            t.axes.get_yaxis().set_visible(False)\n",
        "            plt.imshow(img,cmap='gray_r')\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNS_L9N5dCQY"
      },
      "source": [
        "Returns the number of correct predictions. Note that the output of our model is in one-hot encoding so it has 10 rows and N columns where is the number of data points whereas test_labels is NOT in one-hot encoding so it has a single row and N columns. For a given column i argmax returns the row index which has the largest value, i.e. the largest likelyhood\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhUZc1rDdCQa"
      },
      "source": [
        "def evaluate(test_data,biases,weights):\n",
        "    As=forward_propagation(test_data,biases,weights)\n",
        "    output=As[-1]\n",
        "    count=0\n",
        "    #the output is in one-hot encoding so it has 10 rows\n",
        "    # and number of data columns where as test_tables \n",
        "    # is NOT in one-hot encoding so it has a single row\n",
        "    for i in range(output.shape[0]):\n",
        "        if np.argmax(output[i,:])==test_labels[i,0]:\n",
        "            count=count+1\n",
        "    return count"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4gi1H-dCQn"
      },
      "source": [
        "The usual cross Entropy cost. Unlike the evaluate function both the \"true\" labels and the output of our model are in one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA_ge60UdCQq"
      },
      "source": [
        "def compute_cost(Y,b,w):\n",
        "    \n",
        "    m = Y.shape[0] # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    As=forward_propagation(X,b,w)\n",
        "    # recall that As contains the \"output\" of all layers\n",
        "    # including the input As[0] and the final output As[-1]\n",
        "    output=As[-1]\n",
        "    logprobs = np.log(output)*Y+np.log(1-output)*(1-Y)\n",
        "    cost = -np.sum(logprobs)/m\n",
        "\n",
        "    count=0\n",
        "    for i in range(output.shape[0]):\n",
        "        if (np.argmax(output[i,:])==np.argmax(Y[i,:])):\n",
        "            count=count+1\n",
        "\n",
        "    \n",
        "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
        "                                # E.g., turns [[17]] into 17 \n",
        "    #assert(isinstance(cost, float))\n",
        "    \n",
        "    return cost,count\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQbNBETTdCQ0"
      },
      "source": [
        "The weights are initialized randomly and the biases are initially set to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU6wZ7Q_dCQ2"
      },
      "source": [
        "def initialize_parameters(width):    \n",
        "    weights=[]\n",
        "    biases=[]\n",
        "    for i in range(len(width)-1):\n",
        "        w=np.random.randn(width[i],width[i+1])\n",
        "        b=np.zeros((width[i+1],))\n",
        "        weights.append(w)\n",
        "        biases.append(b)\n",
        "\n",
        "    return biases,weights\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOMuTirBdCQ_"
      },
      "source": [
        "Forward propagation over all the layers but also retain the intermediate results.\n",
        "For example below As[0] is the input, As[1] is the output of the first layer,..., and As[-1] (the last) is the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZy75zCkdCRB"
      },
      "source": [
        "def forward_propagation(X,biases,weights):\n",
        "    a=X\n",
        "    As=[X]\n",
        "    for w,b in zip(weights,biases):\n",
        "       z=np.dot(a,w)+b\n",
        "       a=sigmoid(z)\n",
        "       As.append(a)\n",
        "\n",
        "    return As"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6cM8jih2MHF"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "\\begin{align*}\n",
        "\\Delta^l_{mn}&=\\sum_k \\Delta^{l+1}_{mk}W^l_{nk}\\theta^l_{mn}\\\\\n",
        "dW_{ij}&=\\frac{1}{m}\\sum_m\\Delta^{l+1}_{mj}A^l_{mi}\n",
        "\\end{align*}\n",
        "\n",
        "Or in matrix notation\n",
        "\\begin{align*}\n",
        "\\Delta^l&=\\Delta^{l+1}\\cdot (W^l)^T\\theta^l\\\\\n",
        "dW^l&=\\frac{1}{m}(A^l)^T\\cdot \\Delta^{l+1}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYzG_NTYdCRL"
      },
      "source": [
        "def backward_propagation(X,Y,biases,weights):\n",
        "    \n",
        "    As=forward_propagation(X,biases,weights)\n",
        "    m = X.shape[0]\n",
        "    \n",
        "    nlayers=len(biases)\n",
        "    # Error of the last layer\n",
        "    dz=As[nlayers]-Y\n",
        "\n",
        "    gradb=[]\n",
        "    gradw=[]\n",
        "    for i in range(nlayers,0,-1):\n",
        "        db=np.sum(dz,axis=0,keepdims=True)/m\n",
        "        dw=np.dot(As[i-1].T,dz)/m\n",
        "        dz=np.dot(dz,weights[i-1].T)*As[i-1]*(1-As[i-1])\n",
        "        gradb.insert(0,db)\n",
        "        gradw.insert(0,dw)\n",
        "    \n",
        "    return gradb,gradw"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-9o_pyWdCRY"
      },
      "source": [
        "def update_parameters(biases,weights, gradb,gradw, learning_rate):\n",
        "\n",
        "\n",
        "    for i in range(len(biases)):\n",
        "        weights[i]=weights[i]-learning_rate*gradw[i]\n",
        "        biases[i]=biases[i]-learning_rate*gradb[i]\n",
        "\n",
        "    return biases,weights"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGM-I57dCRj"
      },
      "source": [
        "Initialize the parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0LSeMtvdCRl"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Uo0DJvdCRr"
      },
      "source": [
        "def GD(X, Y, test_data,width,batch_size,num_iterations, learning_rate,print_cost=False):\n",
        "\n",
        "    #n_x=X.shape[0]\n",
        "    #n_y=Y.shape[0]\n",
        "    biases,weights=initialize_parameters(width)\n",
        "    \n",
        "    for i in range(0, num_iterations):\n",
        "        cost,count = compute_cost(Y,biases,weights)\n",
        "        for k in range(0,X.shape[0],batch_size):\n",
        "            idx=[random.randint(0,Y.shape[0]-1) for s in range(batch_size)]\n",
        "            yb=Y[idx,:]\n",
        "            xb=X[idx,:]\n",
        "            gradb,gradw=backward_propagation(xb,yb,biases,weights)\n",
        "            biases,weights=update_parameters(biases,weights,gradb,gradw,learning_rate)    \n",
        "\n",
        "        if i%1 ==0 : \n",
        "            print(len(weights))   \n",
        "            count_test=evaluate(test_data,biases,weights)\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "            print(\"count after iteration %i is %i\" %(i,count))\n",
        "            print(\"test count after iteration %i is %i\" %(i,count_test))\n",
        "    return biases,weights"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "TnZzq_kRdCRw",
        "outputId": "80a08941-f264-469e-ee2d-0d0d987872d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X, Y,test_data,test_labels = load_dataset()\n",
        "X=np.array(X.reshape(60000,784))\n",
        "test_data=np.array(test_data.reshape(10000,784))\n",
        "\n",
        "n_x=X.shape[1]\n",
        "n_y=Y.shape[1]\n",
        "width=[n_x,128,64,n_y]\n",
        "biases,weights= GD(X, Y,test_data,width,batch_size=512,learning_rate=3,\n",
        "                num_iterations =10, print_cost=True)\n",
        "\n",
        "\n",
        "count=evaluate(test_data,biases,weights)\n",
        "A=forward_propagation(X,biases,weights)\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "Cost after iteration 0: 26.315116\n",
            "count after iteration 0 is 5444\n",
            "test count after iteration 0 is 6936\n",
            "3\n",
            "Cost after iteration 1: 1.515225\n",
            "count after iteration 1 is 42117\n",
            "test count after iteration 1 is 8648\n",
            "3\n",
            "Cost after iteration 2: 0.809250\n",
            "count after iteration 2 is 51943\n",
            "test count after iteration 2 is 8939\n",
            "3\n",
            "Cost after iteration 3: 0.645037\n",
            "count after iteration 3 is 53820\n",
            "test count after iteration 3 is 9062\n",
            "3\n",
            "Cost after iteration 4: 0.553171\n",
            "count after iteration 4 is 54849\n",
            "test count after iteration 4 is 9122\n",
            "3\n",
            "Cost after iteration 5: 0.499646\n",
            "count after iteration 5 is 55376\n",
            "test count after iteration 5 is 9149\n",
            "3\n",
            "Cost after iteration 6: 0.460730\n",
            "count after iteration 6 is 55676\n",
            "test count after iteration 6 is 9237\n",
            "3\n",
            "Cost after iteration 7: 0.408860\n",
            "count after iteration 7 is 56274\n",
            "test count after iteration 7 is 9252\n",
            "3\n",
            "Cost after iteration 8: 0.376009\n",
            "count after iteration 8 is 56621\n",
            "test count after iteration 8 is 9304\n",
            "3\n",
            "Cost after iteration 9: 0.351749\n",
            "count after iteration 9 is 56840\n",
            "test count after iteration 9 is 9337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a47ChPTOdCR1"
      },
      "source": [
        "Prints some of the misclassified digits. The top on each digit shows the (wrong) prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM9byvUDdCR2"
      },
      "source": [
        "print_misclassified(test_data,test_labels,biases,weights)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeqLB7jKdCR6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}