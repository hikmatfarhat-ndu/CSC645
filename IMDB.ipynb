{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyisOrUDqLERRektecuEMS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxlaPRhjYIOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "#import cupy as np\n",
        "from keras.utils import to_categorical\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUvFJR-vkYNh",
        "colab_type": "text"
      },
      "source": [
        "## The data\n",
        "\n",
        "THe movie review dataset is a set of 50000 reviews of movies (half training, half test). Each review contains a set of words and is labeled positive (1) or negative (0). For convenience each word index refer to its frequency of occurence in the dataset. For example a word with index 5 is the fifth most frequently used data set. The indices 0,1 and 2 are reserved so 5 really means the third most frequent.\n",
        "\n",
        "Details about the dataset can be found here [Keras IMDB](https://keras.io/api/datasets/imdb/)\n",
        "\n",
        "In this exercise we choose only the first 10000 most frequent words to be included. Any word that is not among them is given the index 2.\n",
        "\n",
        "First we load data set without omitting any words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_34kZwx7bp",
        "colab_type": "text"
      },
      "source": [
        "### Data details\n",
        "\n",
        "We would like to have an idea about the number of reviews, the average length of a review. Also we compute how many entries with values 0,1,2 and 3. The number 0 is used for padding and 1 to denote the beginning of each sequence. The number 2 is used for missing words. Finally, the number 3 is never used since as you will see later we will shift the indices by 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2VxN9cJyNez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "10ab3e1d-a4bb-41f0-e9d7-4eaccca34ee8"
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.imdb.load_data()\n",
        "\n",
        "print(\"The number of reviews in the x_train data set = {}\\n\".format(x_train.shape[0]))\n",
        "print(\"The average length of reviews = {}\".format(np.mean([len(x) for x in x_train])))\n",
        "print(\"With standard deviation = {}\".format(np.std([len(x) for x in x_train])))\n",
        "print(\"The number of 0's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==0])))\n",
        "print(\"The number of 1's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==1])))\n",
        "print(\"The number of 2's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==2])))\n",
        "print(\"The number of 3's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==3])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of reviews in the x_train data set = 25000\n",
            "\n",
            "The average length of reviews = 238.71364\n",
            "With standard deviation = 176.49367364852034\n",
            "The number of 0's in the x_train data set = 0\n",
            "\n",
            "The number of 1's in the x_train data set = 25000\n",
            "\n",
            "The number of 2's in the x_train data set = 1\n",
            "\n",
            "The number of 3's in the x_train data set = 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql1rL9j80jgO",
        "colab_type": "text"
      },
      "source": [
        "Now when we choose only the first 5000 most frequent words and compute the number of 2's in the data set. As you can see the number of 2's is now very large since all the \"ignored\" words were given the code 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBaHMaLXYSQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2da1a654-0ec8-4947-f695-288e69655514"
      },
      "source": [
        "max_words=5000\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_words)\n",
        "print(\"The number of 2's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==2])))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of 2's in the x_train data set = 592372\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Sm_zbolngj",
        "colab_type": "text"
      },
      "source": [
        "### Word index\n",
        "\n",
        "Keras provides also a dictionary of word to index. From that we build a dictionary of index to words. We use the index_to_word to display the first review in the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sKLW9l0YoYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "263efc93-ac55-4864-dc44-34158bd3b206"
      },
      "source": [
        "\n",
        "word_to_index=imdb.get_word_index()\n",
        "print(word_to_index['the'])\n",
        "print(word_to_index['and'])\n",
        "index_to_word=dict([(key,val) for (val,key) in word_to_index.items()])\n",
        "review = \" \".join( [index_to_word.get(i - 3, \"***\") for i in x_train[0]] )\n",
        "\n",
        "\n",
        "\n",
        "def vectorize(sequences, dimension = max_words):\n",
        " results = np.zeros((len(sequences), dimension))\n",
        " for i, sequence in enumerate(sequences):\n",
        "  results[i, sequence] = 1\n",
        " return results\n",
        "\n",
        "x_train=vectorize(x_train)\n",
        "x_test=vectorize(x_test)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zIiD9oiYdLp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "c671a31e-0674-4583-816d-16a4a97de43f"
      },
      "source": [
        "model=tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(1,input_shape=(x_train.shape[1],),activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "model.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=500,\n",
        "    epochs=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 5001      \n",
            "=================================================================\n",
            "Total params: 5,001\n",
            "Trainable params: 5,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6100 - accuracy: 0.7289\n",
            "Epoch 2/2\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.4914 - accuracy: 0.8407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3npX2-Y0px1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=x_train.T\n",
        "x_test=x_test.T"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdM2X-5HVDFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    s = 1/(1+np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtO956vYVELU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \n",
        "#w is a column vector. later on we take the transpose before operating on w\n",
        "    w = np.zeros((1,dim))\n",
        "    b = 0\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAVCNIHzVK2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "# m is the number of samples which is the number of columns in X\n",
        "    m = X.shape[1]\n",
        "    Y_hat= sigmoid(np.dot(w,X)+b)   # compute activation\n",
        "   \n",
        "    cost=-np.sum(Y*np.log(Y_hat)+(1-Y)*np.log(1-Y_hat))/m\n",
        "    # Compute Derivatives\n",
        "\n",
        "    dw = np.dot(X,(Y_hat-Y).T)/m\n",
        "    db = np.sum(Y_hat-Y)/m\n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    return dw, db, cost\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZxEhbLiVPCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(w, b, X, Y, num_iterations,learning_rate, print_cost = False):\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # Cost and gradient calculation \n",
        "        dw,db, cost = propagate(w,b,X,Y)\n",
        "        \n",
        "        # update rule\n",
        "        w = w-learning_rate*dw.T\n",
        "        b = b-learning_rate*db\n",
        "\n",
        "        if print_cost and i % 500 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    \n",
        "    return w,b,dw,db"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4WrorQ-VTUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(w, b, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    \n",
        "    # Compute vector \"Y_hat\" predicting\n",
        "    #    the probabilities of a ship being present in the picture\n",
        "\n",
        "    Y_hat = sigmoid(np.dot(w,X)+b)\n",
        "\n",
        "    for i in range(Y_hat.shape[1]):\n",
        "        \n",
        "        # Convert probabilities Y_hat[0,i] to actual predictions p[0,i]\n",
        "        if Y_hat[0,i]>=0.5:\n",
        "            Y_prediction[0,i]=1\n",
        "        else:\n",
        "            Y_prediction[0,i]=0\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "\n",
        "    return Y_prediction"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8RZ-AK607lV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize parameters with zeros \n",
        "w, b = initialize_with_zeros(x_train.shape[0])\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Estuo_NVXr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8a4f3562-fb6c-4c74-9e91-032c5b3ab85f"
      },
      "source": [
        "num_iterations = 2000\n",
        "learning_rate = 0.2\n",
        "print_cost = True\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Gradient descent \n",
        "w,b,dw,db= learn(w, b, x_train, y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "\n",
        "# Predict test/train set examples\n",
        "Y_prediction_test = predict(w, b, x_test)\n",
        "Y_prediction_train = predict(w, b, x_train)\n",
        "\n",
        "# Print train/test Errors\n",
        "\n",
        "\n",
        "print(\"train accuracy:\"+str((100 - np.mean(np.abs(Y_prediction_train - y_train)) * 100)))\n",
        "print(\"test accuracy:\"+str((100 - np.mean(np.abs(Y_prediction_test - y_test)) * 100)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 500: 0.302174\n",
            "Cost after iteration 1000: 0.266001\n",
            "Cost after iteration 1500: 0.247101\n",
            "train accuracy:91.428\n",
            "test accuracy:88.3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}