{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPogPhg/WQSpoGquqUfqhGc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxlaPRhjYIOj"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import numpy as np\n",
        "#import cupy as np\n",
        "from keras.utils import to_categorical\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKGBac0XV6uH",
        "outputId": "5d27ea02-4bc2-4d0f-995c-0f43a798b8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(6, 1, input_length=4))\n",
        "#model.add(tf.keras.layers.Flatten())\n",
        "#input_array = np.random.randint(10, size=(3, 4))\n",
        "input_array=np.array([[1,2,3,4],[4,3,2,1],[3,3,4,5]])\n",
        "model.compile('rmsprop', 'mse')\n",
        "output_array = model.predict(input_array)\n",
        "print(output_array.shape)\n",
        "print(output_array)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0be49d8bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "(3, 4, 1)\n",
            "[[[ 0.03702595]\n",
            "  [-0.00937339]\n",
            "  [ 0.02088351]\n",
            "  [ 0.01764253]]\n",
            "\n",
            " [[ 0.01764253]\n",
            "  [ 0.02088351]\n",
            "  [-0.00937339]\n",
            "  [ 0.03702595]]\n",
            "\n",
            " [[ 0.02088351]\n",
            "  [ 0.02088351]\n",
            "  [ 0.01764253]\n",
            "  [ 0.0463757 ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUvFJR-vkYNh"
      },
      "source": [
        "## The data\n",
        "\n",
        "THe movie review dataset is a set of 50000 reviews of movies (half training, half test). Each review contains a set of words and is labeled positive (1) or negative (0). For convenience each word index refer to its frequency of occurence in the dataset. For example a word with index 5 is the fifth most frequently used data set. The indices 0,1 and 2 are reserved so 5 really means the third most frequent.\n",
        "\n",
        "Details about the dataset can be found here [Keras IMDB](https://keras.io/api/datasets/imdb/)\n",
        "\n",
        "In this exercise we choose only the first 10000 most frequent words to be included. Any word that is not among them is given the index 2.\n",
        "\n",
        "First we load data set without omitting any words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX_34kZwx7bp"
      },
      "source": [
        "### Data details\n",
        "\n",
        "We would like to have an idea about the number of reviews, the average length of a review. Also we compute how many entries with values 0,1,2 and 3. The number 0 is used for padding and 1 to denote the beginning of each sequence. The number 2 is used for missing words. Finally, the number 3 is never used since as you will see later we will shift the indices by 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2VxN9cJyNez",
        "outputId": "1c0868aa-c4c6-49d2-8377-791a272e8536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "(x_train,y_train),(x_test,y_test)=tf.keras.datasets.imdb.load_data()\n",
        "\n",
        "print(\"The number of reviews in the x_train data set = {}\\n\".format(x_train.shape[0]))\n",
        "print(\"The average length of reviews = {}\".format(np.mean([len(x) for x in x_train])))\n",
        "print(\"With standard deviation = {}\".format(np.std([len(x) for x in x_train])))\n",
        "print(\"The number of 0's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==0])))\n",
        "print(\"The number of 1's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==1])))\n",
        "print(\"The number of 2's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==2])))\n",
        "print(\"The number of 3's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==3])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of reviews in the x_train data set = 25000\n",
            "\n",
            "The average length of reviews = 238.71364\n",
            "With standard deviation = 176.49367364852034\n",
            "The number of 0's in the x_train data set = 0\n",
            "\n",
            "The number of 1's in the x_train data set = 25000\n",
            "\n",
            "The number of 2's in the x_train data set = 1\n",
            "\n",
            "The number of 3's in the x_train data set = 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql1rL9j80jgO"
      },
      "source": [
        "Now when we choose only the first max_words most frequent words and compute the number of 2's in the data set. As you can see the number of 2's is now very large since all the \"ignored\" words were given the code 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBaHMaLXYSQT",
        "outputId": "136a5fab-adb9-449a-ead3-752ff0f3b418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "max_words=5000\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_words)\n",
        "print(\"The number of 2's in the x_train data set = {}\\n\".format(sum([1 for x in np.hstack(x_train) if x==2])))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of 2's in the x_train data set = 592372\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Sm_zbolngj"
      },
      "source": [
        "### Word index\n",
        "\n",
        "Keras provides also a dictionary of word to index. From that we build a dictionary of index to words. We use the index_to_word to display the first review in the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sKLW9l0YoYE"
      },
      "source": [
        "\n",
        "word_to_index=imdb.get_word_index()\n",
        "index_to_word=dict([(key,val) for (val,key) in word_to_index.items()])\n",
        "review = \" \".join( [index_to_word.get(i - 3, \"***\") for i in x_train[0]] )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYprPNE5S-LD",
        "outputId": "ff25f8a1-5b18-4863-e5f8-15e42451cdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "def vectorize(sequences, dimension = max_words):\n",
        " results = np.zeros((len(sequences), dimension))\n",
        " for i, sequence in enumerate(sequences):\n",
        "  results[i, sequence] = 1\n",
        " return results\n",
        "\n",
        "x_train=vectorize(x_train)\n",
        "x_test=vectorize(x_test)\n",
        "print(x_train.shape)\n",
        "#tmp_train=pad_sequences(x_train,maxlen=500)\n",
        "#tmp_test=pad_sequences(x_test,maxlen=500)\n",
        "#tmp_train=tf.keras.layers.Embedding(5000,5000,input_length=500)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYBCzczk8afa",
        "outputId": "b072dac4-59f1-4762-ad2a-44fd7117cc4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        }
      },
      "source": [
        "x_train=pad_sequences(x_train,maxlen=500)\n",
        "x_test=pad_sequences(x_test,maxlen=500)\n",
        "model=tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_words,1,input_length=500))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "model.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=500,\n",
        "    epochs=20)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 500, 1)            5000      \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 501       \n",
            "=================================================================\n",
            "Total params: 5,501\n",
            "Trainable params: 5,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.6931 - accuracy: 0.5079\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.6887 - accuracy: 0.5791\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.6637 - accuracy: 0.6838\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.5978 - accuracy: 0.7560\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.5150 - accuracy: 0.8059\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.4450 - accuracy: 0.8393\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.3944 - accuracy: 0.8602\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.3585 - accuracy: 0.8707\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.3321 - accuracy: 0.8788\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.3114 - accuracy: 0.8872\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2943 - accuracy: 0.8925\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2805 - accuracy: 0.8974\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2686 - accuracy: 0.9026\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2589 - accuracy: 0.9058\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2496 - accuracy: 0.9089\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 0s 8ms/step - loss: 0.2420 - accuracy: 0.9120\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.2351 - accuracy: 0.9150\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.2283 - accuracy: 0.9175\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.2225 - accuracy: 0.9192\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 0s 9ms/step - loss: 0.2173 - accuracy: 0.9217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zIiD9oiYdLp",
        "outputId": "b2443b0f-ba51-4ea6-bb45-1ff1f4ce03a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "model=tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(1,input_shape=(x_train.shape[1],),activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "model.compile(optimizer=\"Adam\",loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=500,\n",
        "    epochs=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 5001      \n",
            "=================================================================\n",
            "Total params: 5,001\n",
            "Trainable params: 5,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.6142 - accuracy: 0.7194\n",
            "Epoch 2/2\n",
            "50/50 [==============================] - 0s 5ms/step - loss: 0.4948 - accuracy: 0.8416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3npX2-Y0px1"
      },
      "source": [
        "x_train=x_train.T\n",
        "x_test=x_test.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdM2X-5HVDFZ"
      },
      "source": [
        "def sigmoid(z):\n",
        "    s = 1/(1+np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtO956vYVELU"
      },
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \n",
        "#w is a column vector. later on we take the transpose before operating on w\n",
        "    w = np.zeros((1,dim))\n",
        "    b = 0\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAVCNIHzVK2O"
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "# m is the number of samples which is the number of columns in X\n",
        "    m = X.shape[1]\n",
        "    Y_hat= sigmoid(np.dot(w,X)+b)   # compute activation\n",
        "   \n",
        "    cost=-np.sum(Y*np.log(Y_hat)+(1-Y)*np.log(1-Y_hat))/m\n",
        "    # Compute Derivatives\n",
        "\n",
        "    dw = np.dot(X,(Y_hat-Y).T)/m\n",
        "    db = np.sum(Y_hat-Y)/m\n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    return dw, db, cost\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZxEhbLiVPCw"
      },
      "source": [
        "def learn(w, b, X, Y, num_iterations,learning_rate, print_cost = False):\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        # Cost and gradient calculation \n",
        "        dw,db, cost = propagate(w,b,X,Y)\n",
        "        \n",
        "        # update rule\n",
        "        w = w-learning_rate*dw.T\n",
        "        b = b-learning_rate*db\n",
        "\n",
        "        if print_cost and i % 500 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    \n",
        "    return w,b,dw,db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4WrorQ-VTUD"
      },
      "source": [
        "def predict(w, b, X):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    \n",
        "    # Compute vector \"Y_hat\" predicting\n",
        "    #    the probabilities of a ship being present in the picture\n",
        "\n",
        "    Y_hat = sigmoid(np.dot(w,X)+b)\n",
        "\n",
        "    for i in range(Y_hat.shape[1]):\n",
        "        \n",
        "        # Convert probabilities Y_hat[0,i] to actual predictions p[0,i]\n",
        "        if Y_hat[0,i]>=0.5:\n",
        "            Y_prediction[0,i]=1\n",
        "        else:\n",
        "            Y_prediction[0,i]=0\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "\n",
        "    return Y_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8RZ-AK607lV"
      },
      "source": [
        "# initialize parameters with zeros \n",
        "w, b = initialize_with_zeros(x_train.shape[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Estuo_NVXr1",
        "outputId": "8a4f3562-fb6c-4c74-9e91-032c5b3ab85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "num_iterations = 2000\n",
        "learning_rate = 0.2\n",
        "print_cost = True\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "# Gradient descent \n",
        "w,b,dw,db= learn(w, b, x_train, y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "\n",
        "# Predict test/train set examples\n",
        "Y_prediction_test = predict(w, b, x_test)\n",
        "Y_prediction_train = predict(w, b, x_train)\n",
        "\n",
        "# Print train/test Errors\n",
        "\n",
        "\n",
        "print(\"train accuracy:\"+str((100 - np.mean(np.abs(Y_prediction_train - y_train)) * 100)))\n",
        "print(\"test accuracy:\"+str((100 - np.mean(np.abs(Y_prediction_test - y_test)) * 100)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693147\n",
            "Cost after iteration 500: 0.302174\n",
            "Cost after iteration 1000: 0.266001\n",
            "Cost after iteration 1500: 0.247101\n",
            "train accuracy:91.428\n",
            "test accuracy:88.3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}