{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "sequence-modeling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/sequence_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13NLGKdqH92v",
        "outputId": "faede63a-577d-4665-f220-6b472ce40d4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p64nYl4lH920"
      },
      "source": [
        "# Sequence modelling \n",
        "\n",
        "## Coding tutorials\n",
        " #### [1.  The IMDb dataset](#coding_tutorial_1)\n",
        " #### [2. Padding and masking sequence data](#coding_tutorial_2)\n",
        " #### [3. The Embedding layer](#coding_tutorial_3)\n",
        " #### [4. The Embedding Projector](#coding_tutorial_4)\n",
        " #### [5. Recurrent neural network layers](#coding_tutorial_5)\n",
        " #### [6. Stacked RNNs and the Bidirectional wrapper](#coding_tutorial_6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfGnxJ_PH921"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## The IMDb Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR-iKXE4H922"
      },
      "source": [
        "#### Load the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYAlPv_3H922"
      },
      "source": [
        "# Import imdb\n",
        "import tensorflow.keras.datasets.imdb as imdb "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lWf_skeH927"
      },
      "source": [
        "# Download and assign the data set using load_data()\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QByeFp1-H929"
      },
      "source": [
        "#### Inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOKcazZ-H929"
      },
      "source": [
        "# Inspect the type of the data\n",
        "x_train[0][0:10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAmh7RBPH93A"
      },
      "source": [
        "# Inspect the shape of the data\n",
        "\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M13T9MYmH93D"
      },
      "source": [
        "# Display the first dataset element input\n",
        "# Notice encoding\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vC8tfilH93F"
      },
      "source": [
        "# Display the first dataset element output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsBkT84WH93H"
      },
      "source": [
        "#### Load dataset with different options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FibusPh6H93I"
      },
      "source": [
        "# Load the dataset with defaults\n",
        "\n",
        "\n",
        "# ~/.keras/dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUqaib9vH93K"
      },
      "source": [
        "# Limit the vocabulary to the top 500 words using num_words\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY0nVpL5H93L"
      },
      "source": [
        "# Ignore the top 10 most frequent words using skip_top\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyJoRvxnH93O"
      },
      "source": [
        "# Limit the sequence lengths to 500 using maxlen\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGl2t4FzH93Q"
      },
      "source": [
        "# Use '1' as the character that indicates the start of a sequence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV5FtNUNH93T"
      },
      "source": [
        "#### Explore the dataset word index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JPbqBjIH93U"
      },
      "source": [
        "# Load the imdb word index using get_word_index()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjSZn6FxH93W"
      },
      "source": [
        "# View the word index as a dictionary,\n",
        "# accounting for index_from.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pNrOLrHH93Y"
      },
      "source": [
        "# Retrieve a specific word's index\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QHwuY7vH93a"
      },
      "source": [
        "# View an input sentence\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d7zj8y0H93c"
      },
      "source": [
        "# Get the sentiment value\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHVV1zb9H93f"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Padding and Masking Sequence Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BCmQvaBH93f"
      },
      "source": [
        "# Load the imdb data set\n",
        "(x_train,y_train),(x_test,y_test)=imdb.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkvZ0PIfH93i"
      },
      "source": [
        "#### Preprocess the data with padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCr27NoEH93j"
      },
      "source": [
        "# Inspect the input data shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMX6ScXMH93l"
      },
      "source": [
        "# Pad the inputs to the maximum length using maxlen\n",
        "\n",
        "x_train_pad=tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=300,\n",
        "padding='post',truncating='pre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "213GcrjfH93n"
      },
      "source": [
        "# Inspect the output data shape\n",
        "\n",
        "x_train_pad.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDH66f7KH93p"
      },
      "source": [
        "#### Create a Masking layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84PuXe5zH93p"
      },
      "source": [
        "# Import numpy \n",
        "import numpy  as np \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGauvsn1H93s"
      },
      "source": [
        "# Masking expects to see (batch, sequence, features)\n",
        "# Create a dummy feature dimension using expand_dims\n",
        "padded_x_train=np.expand_dims(x_train_pad,-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5HXuT0aH93u"
      },
      "source": [
        "# Create a Masking layer \n",
        "tf_x_train=tf.convert_to_tensor(padded_x_train,dtype='float32')\n",
        "mask_layer=tf.keras.layers.Masking(mask_value=0,)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rekywi0dH93y"
      },
      "source": [
        "# Pass tf_x_train to it\n",
        "masked_x_train=mask_layer(tf_x_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmkU8oOHH931"
      },
      "source": [
        "# Look at the dataset\n",
        "masked_x_train.shape\n",
        "masked_x_train._keras_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ORzULiIH933"
      },
      "source": [
        "# Look at the ._keras_mask for the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qcZxQ4SH935"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## The Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giGe8bCBH935"
      },
      "source": [
        "#### Create and apply an `Embedding` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKwfU2IcH936"
      },
      "source": [
        "# Create an embedding layer using layers.Embedding\n",
        "# Specify input_dim, output_dim, input_length\n",
        "\n",
        "embedding_layer=tf.keras.layers.Embedding(input_dim=501,output_dim=16)\n",
        "#expects input of the shape\n",
        "# (batch,sequence,features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgA8jGwcH938"
      },
      "source": [
        "# Inspect an Embedding layer output for a fixed input\n",
        "# Expects an input of shape (batch, sequence, feature)\n",
        "sequence=tf.constant([[[0],[1],[5],[500]]])\n",
        "sequence_of_embeddings=embedding_layer(sequence)\n",
        "sequence_of_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAlsPukyH93_"
      },
      "source": [
        "# Inspect the Embedding layer weights using get_weights()\n",
        "# in this case it returns a list of just one element\n",
        "w=embedding_layer.get_weights()\n",
        "w[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8xX0jEwH94A"
      },
      "source": [
        "# Get the embedding for the 14th index\n",
        "embedding_layer.get_weights()[0][0,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM55i5DjH94E"
      },
      "source": [
        "#### Create and apply an `Embedding` layer that uses `mask_zero=True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKaN5gnEH94E"
      },
      "source": [
        "# Create a layer that uses the mask_zero kwarg\n",
        "masking_embedding_layer=tf.keras.layers.Embedding(input_dim=501,output_dim=16,mask_zero=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIC7QX7PH94G"
      },
      "source": [
        "# Apply this layer to the sequence and see the _keras_mask property\n",
        "masked_sequence=masking_embedding_layer(sequence)\n",
        "masked_sequence._keras_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEWIYD8YH94I"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## The Embedding Projector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_KaAU1kIMRi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urtrTWQ2H94I"
      },
      "source": [
        "#### Load and preprocess the IMDb data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IStW5oGH94I"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50bjCHWAH94L"
      },
      "source": [
        "# Load the dataset\n",
        "(x_train,y_train),(x_test,y_test)=get_and_pad_imdb_dataset()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRMfQ0gtH94P"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6wj7MNvH94Q"
      },
      "source": [
        "# Get the word index\n",
        "imdb_word_index=get_imdb_word_index()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPBKx4yrH94S"
      },
      "source": [
        "# Swap the keys and values of the word index\n",
        "inv_imdb_word_index={val:key for key,val in imdb_word_index.items()}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64e5dj9KH94V"
      },
      "source": [
        "# View the first dataset example sentence\n",
        "[inv_imdb_word_index[index] for index in x_train[0] if index >2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEWW_bGEH94Y"
      },
      "source": [
        "#### Build an Embedding layer into a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ZsWSTrH94Z"
      },
      "source": [
        "# Get the maximum token value\n",
        "\n",
        "max_index_value=max(imdb_word_index.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc1ctkHYH94c"
      },
      "source": [
        "# Specify an embedding dimension\n",
        "embedding_dim=16\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yByNCHnH94f"
      },
      "source": [
        "# Build a model using Sequential:\n",
        "#     1. Embedding layer\n",
        "#     2. GlobalAveragePooling1D\n",
        "#     3. Dense\n",
        "model=tf.keras.Sequential([\n",
        "           tf.keras.layers.Embedding(input_dim=max_index_value+1,output_dim=embedding_dim,mask_zero=False)  ,\n",
        "           tf.keras.layers.GlobalAveragePooling1D(),\n",
        "           tf.keras.layers.Dense(units=1,activation='sigmoid')              \n",
        "\n",
        "]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ts-QXwKH94h"
      },
      "source": [
        "# Functional API refresher: use the Model to build the same model\n",
        "\n",
        "review_sequence=tf.keras.Input((None,))\n",
        "embedding_sequence=tf.keras.layers.Embedding(input_dim=max_index_value+1,output_dim=embedding_dim)(review_sequence)\n",
        "average_embedding=tf.keras.layers.GlobalAveragePooling1D()(embedding_sequence)\n",
        "positive_prob=tf.keras.layers.Dense(1,activation='sigmoid')(average_embedding)\n",
        "model=tf.keras.Model(inputs=review_sequence,outputs=positive_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp0JnxbQH94k"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZIMFki6H94n"
      },
      "source": [
        "#### Compile, train, and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPddJnO5H94n"
      },
      "source": [
        "# Compile the model with a binary cross-entropy loss\n",
        "model.compile(loss='binary_crossentropy',metrics='accuracy',optimizer='adam')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDFU38UIH94p"
      },
      "source": [
        "# Train the model using .fit(), savng its history\n",
        "\n",
        "history=model.fit(x_train,y_train,epochs=5,batch_size=32,validation_data=(x_test,y_test),validation_steps=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3AO11jOH94r"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ikTcWoBH94u"
      },
      "source": [
        "#### The TensorFlow embedding projector\n",
        "\n",
        "The Tensorflow embedding projector can be found [here](https://projector.tensorflow.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_cYLBIiH94v"
      },
      "source": [
        "# Retrieve the embedding layer's weights from the trained model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_P4_jCoH94w"
      },
      "source": [
        "# Save the word Embeddings to tsv files\n",
        "# Two files: \n",
        "#     one contains the embedding labels (meta.tsv),\n",
        "#     one contains the embeddings (vecs.tsv)\n",
        "\n",
        "import io\n",
        "from os import path\n",
        "\n",
        "out_v = io.open(path.join('data', 'vecs.tsv'), 'w', encoding='utf-8')\n",
        "out_m = io.open(path.join('data', 'meta.tsv'), 'w', encoding='utf-8')\n",
        "\n",
        "k = 0\n",
        "\n",
        "for word, token in word_index.items():\n",
        "    if k != 0:\n",
        "        out_m.write('\\n')\n",
        "        out_v.write('\\n')\n",
        "    \n",
        "    out_v.write('\\t'.join([str(x) for x in weights[token]]))\n",
        "    out_m.write(word)\n",
        "    k += 1\n",
        "    \n",
        "out_v.close()\n",
        "out_m.close()\n",
        "# beware large collections of embeddings!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJPQtIZdH944"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_5\"></a>\n",
        "## Recurrent neural network layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQDxIriRH944"
      },
      "source": [
        "#### Initialize and pass an input to a SimpleRNN layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBJ8a5vzH945"
      },
      "source": [
        "# Create a SimpleRNN layer and test it\n",
        "\n",
        "# input of the form (batch,sequence,features)\n",
        "simplernn_layers=tf.keras.layers.SimpleRNN(units=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDl9hK-8H947"
      },
      "source": [
        "# Note that only the final cell output is returned\n",
        "sequence=tf.constant([[[1.,1.],[2.,2.],[56.,-100]]])\n",
        "layer_output=simplernn_layers(sequence)\n",
        "layer_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nH0tWhEH95A"
      },
      "source": [
        "#### Load and transform the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR-uHxhiH95B"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rwd0mwJkH95D",
        "outputId": "ee185d18-7239-4b7f-a07f-8744e3385f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the dataset\n",
        "num_words=5000\n",
        "maxlen=300\n",
        "(x_train,y_train),(x_test,y_test)=get_and_pad_imdb_dataset(num_words=num_words,maxlen=300)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPANcwE1H95H"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=5000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvuhek9BH95J",
        "outputId": "5a6964ad-1c9c-40a9-832f-7f4f37f7852c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the word index using get_imdb_word_index()\n",
        "imdb_word_index=get_imdb_word_index(num_words=num_words)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aav965QDH95L"
      },
      "source": [
        "#### Create a recurrent neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9ltGa8pH95L"
      },
      "source": [
        "# Get the maximum index value\n",
        "\n",
        "max_index_value=max(imdb_word_index.values())\n",
        "embedding_dim=128"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FHM89-4H95O"
      },
      "source": [
        "# Using Sequential, build the model:\n",
        "# 1. Embedding.\n",
        "# 2. LSTM.\n",
        "# 3. Dense.\n",
        "model=tf.keras.Sequential([\n",
        "     tf.keras.layers.Embedding(input_dim=max_index_value+1,output_dim=embedding_dim,mask_zero=True),\n",
        "     tf.keras.layers.Dropout(0.3),\n",
        "     tf.keras.layers.LSTM(units=60),\n",
        "     tf.keras.layers.Dropout(0.3),\n",
        "     tf.keras.layers.Dense(units=1,activation='sigmoid')                \n",
        "])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWPRGm6sH95R"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zotvkXSfH95R"
      },
      "source": [
        "# Compile the model with binary cross-entropy loss\n",
        "\n",
        "model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgUS_2WIH95T",
        "outputId": "e1750e14-610a-43ba-d841-4d9f60df1fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fit the model and save its training history\n",
        "history=model.fit(x_train,y_train,epochs=5,batch_size=500,validation_data=(x_test,y_test))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "39/39 [==============================] - 29s 731ms/step - loss: 0.6232 - accuracy: 0.6613 - val_loss: 0.4467 - val_accuracy: 0.7893\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 27s 701ms/step - loss: 0.3323 - accuracy: 0.8656 - val_loss: 0.3206 - val_accuracy: 0.8653\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 27s 696ms/step - loss: 0.2515 - accuracy: 0.9047 - val_loss: 0.3066 - val_accuracy: 0.8733\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 27s 703ms/step - loss: 0.2092 - accuracy: 0.9242 - val_loss: 0.3788 - val_accuracy: 0.8579\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 28s 717ms/step - loss: 0.1887 - accuracy: 0.9325 - val_loss: 0.3230 - val_accuracy: 0.8694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpiwIFlfOo0b"
      },
      "source": [
        "model.evaluate(x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNMVsBFsH95V"
      },
      "source": [
        "#### Plot learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faKUwy8DH95V"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blC-Bzv4H95Y"
      },
      "source": [
        "#### Make predictions with the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT0y-viOH95Y"
      },
      "source": [
        "# View the first test data example sentence\n",
        "# (invert the word index)\n",
        "inv_imdb_word_index={val:key for key,val in imdb_word_index.items()}\n",
        "[inv_imdb_word_index[index] for index in x_test[0] if index >2]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqhocrzVH95a"
      },
      "source": [
        "# Get the model prediction using model.predict()\n",
        "\n",
        "model.predict(x_test[None,0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5n2s8TPH95b"
      },
      "source": [
        "# Get the corresponding label\n",
        "y_test[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kelDHTwH95d"
      },
      "source": [
        "---\n",
        "<a id=\"coding_tutorial_6\"></a>\n",
        "## Stacked RNNs and the Bidirectional wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9hOLaF0H95d"
      },
      "source": [
        "#### Load and transform the IMDB review sentiment dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlhC0kNeH95d"
      },
      "source": [
        "# A function to load and preprocess the IMDB dataset\n",
        "\n",
        "def get_and_pad_imdb_dataset(num_words=10000, maxlen=None, index_from=2):\n",
        "    from tensorflow.keras.datasets import imdb\n",
        "\n",
        "    # Load the reviews\n",
        "    (x_train, y_train), (x_test, y_test) = imdb.load_data(path='imdb.npz',\n",
        "                                                          num_words=num_words,\n",
        "                                                          skip_top=0,\n",
        "                                                          maxlen=maxlen,\n",
        "                                                          start_char=1,\n",
        "                                                          oov_char=2,\n",
        "                                                          index_from=index_from)\n",
        "\n",
        "    x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,\n",
        "                                                        maxlen=None,\n",
        "                                                        padding='pre',\n",
        "                                                        truncating='pre',\n",
        "                                                        value=0)\n",
        "    \n",
        "    x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
        "                                                           maxlen=None,\n",
        "                                                           padding='pre',\n",
        "                                                           truncating='pre',\n",
        "                                                           value=0)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FA1lEYgH95f"
      },
      "source": [
        "# Load the dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_rgVNe8H95h"
      },
      "source": [
        "# A function to get the dataset word index\n",
        "\n",
        "def get_imdb_word_index(num_words=10000, index_from=2):\n",
        "    imdb_word_index = tf.keras.datasets.imdb.get_word_index(\n",
        "                                        path='imdb_word_index.json')\n",
        "    imdb_word_index = {key: value + index_from for\n",
        "                       key, value in imdb_word_index.items() if value <= num_words-index_from}\n",
        "    return imdb_word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1BEiLjhH95k"
      },
      "source": [
        "# Get the word index using get_imdb_word_index()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71zZ2hxMH95l"
      },
      "source": [
        "#### Build stacked and bidirectional recurrent models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61evA537H95m"
      },
      "source": [
        "# Get the maximum index value and specify an embedding dimension\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6zDPq1nH95o"
      },
      "source": [
        "# Using Sequential, build a stacked LSTM model via return_sequences=True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uxWnYT9H95q"
      },
      "source": [
        "# Using Sequential, build a bidirectional RNN with merge_mode='sum'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exo2n5vgH95r"
      },
      "source": [
        "# Create a model featuring both stacked recurrent layers and a bidirectional layer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ8Bv801H95s"
      },
      "source": [
        "#### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3CpQN3pH95t"
      },
      "source": [
        "# Compile the model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiuQLrCrH95u"
      },
      "source": [
        "# Train the model, saving its history\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeRESIxuH95v"
      },
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc      = history_dict['accuracy']\n",
        "val_acc  = history_dict['val_accuracy']\n",
        "loss     = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(epochs, acc, marker='.', label='Training acc')\n",
        "plt.plot(epochs, val_acc, marker='.', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Classification accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}