{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "shallow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hikmatfarhat-ndu/CSC645/blob/master/shallow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHNba3P7AjEZ"
      },
      "source": [
        "# A shallow (two layers) network- Recognizing Sonar data\n",
        "In this exercises we will use a **two layer** (1 input, 1 hidden and 1 output) neural network to classify a two-class **sonar** data. Each entry is the result of bouncing off sonar signal from different angles at metals cylinder (Mines) and rock (Rock) objects. It contains 60 values between 0 and 1 and a corresponding label (M or R). A detailed description of the data set can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB1NCGg_AjEd"
      },
      "source": [
        "### Importing packages\n",
        "We need the follwing packages: numpy for the computation, google.colab for loading the data file into the colab notebook and finally pandas for reading the data from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MFSVd9VAjEg"
      },
      "source": [
        "import numpy as np\n",
        "#from google.colab import files\n",
        "import pandas as pd\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWygj19pAjE1"
      },
      "source": [
        "### Reading the data\n",
        "Upload the data file to colab and read it using the pandas package. Note the ! allows us to run any shell command from the notebook. Also we need the __unformatted__ file from github so we use the raw content link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXRfaXGsqtjA",
        "outputId": "ed45882b-0d18-44b9-c061-a28659ca06b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#file=files.upload()\n",
        "!wget https://raw.githubusercontent.com/hikmatfarhat-ndu/CSC645/master/sonar.csv\n",
        "df=pd.read_csv(\"sonar.csv\",header=None)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-31 16:21:31--  https://raw.githubusercontent.com/hikmatfarhat-ndu/CSC645/master/sonar.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87776 (86K) [text/plain]\n",
            "Saving to: ‘sonar.csv.3’\n",
            "\n",
            "\rsonar.csv.3           0%[                    ]       0  --.-KB/s               \rsonar.csv.3         100%[===================>]  85.72K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2020-10-31 16:21:31 (21.2 MB/s) - ‘sonar.csv.3’ saved [87776/87776]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfiIRkPYJNFE"
      },
      "source": [
        "## Looking at the data\n",
        "\n",
        "Next we print the shape of the imported data. As you can see below all the Rocks are grouped together followed by the Mines grouped together. Since we have a single set of 208 samples later will will need to use a portion for testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEte8kiXHCIg",
        "outputId": "0264d790-accc-4187-c55e-8b9fa862d96a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "print(\"the data has {} rows and {} columns\".format(df.shape[0],df.shape[1]))\n",
        "print(\"we will view the first few and last few rows/columns\")\n",
        "cols=[1,2,59,60]\n",
        "rows=[1,2,206,207]\n",
        "df.loc[rows,cols]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the data has 208 rows and 61 columns\n",
            "we will view the first few and last few rows/columns\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>0.0353</td>\n",
              "      <td>0.0490</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>0.0363</td>\n",
              "      <td>0.0136</td>\n",
              "      <td>0.0115</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         1       2       59 60\n",
              "1    0.0523  0.0843  0.0044  R\n",
              "2    0.0582  0.1099  0.0078  R\n",
              "206  0.0353  0.0490  0.0048  M\n",
              "207  0.0363  0.0136  0.0115  M"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdtWZ0JK0nEo"
      },
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "Before we start the learning process we need to preprocess the data. First, all the Mines \"M\" are grouped together and the Rocks \"R\" are grouped together as can be seen from the output of the previous cell. We use the numpy __shuffle__ function to mix them randomly. Second, Pandas reads the data as pandas frame so we need to extract the data values and the label values. Third, we convert the labels from \"M\" to 1 and from \"R\" to 0. Finally, we divide the data set into training and test subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA9AdUF3q3uH",
        "tags": [],
        "outputId": "32af8353-866f-4aff-cd87-0159809c9b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#pandas data frame\n",
        "m=df.values\n",
        "# randomize (shuffle) the data\n",
        "np.random.shuffle(m)\n",
        "#m=m.T\n",
        "# Each row has 61 entries, 60 for data and the last one is the label \"M\" or \"R\"\n",
        "\n",
        "# X contains all the data\n",
        "X=m[:,0:60].astype(\"float32\")\n",
        "\n",
        "# Y contains all the labels\n",
        "\n",
        "Y=m[:,60]\n",
        "# convert the labels: \"M\"->1 and \"R\"->0\n",
        "Y=np.array([1.0 if i=='M' else 0.0 for i in Y])\n",
        "#Y=Y.reshape((1,len(Y)))\n",
        "Y=Y.reshape((len(Y),1))\n",
        "Y=Y.astype(\"float32\")\n",
        "\n",
        "# split the data and labels into a training and test sets\n",
        "train_size=180\n",
        "data_size=X.shape[0]\n",
        "#x_train=X[:,0:train_size]\n",
        "x_train=X[0:train_size,:]\n",
        "#x_test=X[:,train_size:data_size]\n",
        "x_test=X[train_size:data_size,:]\n",
        "#y_train=Y[:,0:train_size]\n",
        "y_train=Y[0:train_size,:]\n",
        "#y_test=Y[:,train_size:data_size]\n",
        "y_test=Y[train_size:data_size,:]\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(180, 60)\n",
            "(28, 60)\n",
            "(180, 1)\n",
            "(28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5WG9zM11JvV"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pZo-cdyAjFl"
      },
      "source": [
        "learning_rate = 3\n",
        "nb_iterations = 5000\n",
        "# Network Parameters\n",
        "n_h = 64 # number of neurons in hidden layer\n",
        "n_x = x_train.shape[1] #number of neurons in input\n",
        "n_y = y_train.shape[1] #number of neurons in ouput"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-nWNb3XAjFn"
      },
      "source": [
        "![alt text](shallow-example.png \"Title\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r41rVrvQAjFp"
      },
      "source": [
        "### Sigmoid function\n",
        "First write the sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nPwBnlHAjFq"
      },
      "source": [
        "def sigmoid(z):\n",
        "    s=1/(1+np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDqKwr4MAjF3"
      },
      "source": [
        "### Initializing the parameters\n",
        "Since we have two layers we will need two weight matrices and two bias vectors. Consult the forward propagation equations shown below to be able to determine the shape of the parameters and therefore initialize them.\n",
        "$\\sigma$ is the sigmoid function defined above, $A^0=X$ is the input, $A^1$ and $A^2$ are the output of the first and second layers respectively. Recall that all the variables below (except the parameters) are vectorized version containing all the samples where the samples are column stacked. So X[:,0] is the input of the first (0) sample\n",
        "Z1[0,0] is the output of the first node in the first layer when the input is the first sample, etc..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f80RjsXsAjF5"
      },
      "source": [
        "\\begin{align*}\n",
        "    Z^1&=W^0\\cdot A^0+B^0\\\\\n",
        "    A^1&=\\sigma(Z^1)\\\\\n",
        "    Z^2&=W^1\\cdot A^1+B^1\\\\\n",
        "    A^2&=\\sigma(Z^2)\n",
        "  \\end{align*}\n",
        "We initialize the weights randomly and the biases to zero. This is done in numpy by using the random.randn and zeros functions. To create an nxm matrix of random numbers we use np.random.randn(n,m) and to create an nxm matrix of zeros we use np.zeros((n,m))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwngNIdsAjF7"
      },
      "source": [
        "\n",
        "W0=np.random.randn(n_x,n_h)\n",
        "b0=np.zeros((n_h))\n",
        "W1=np.random.randn(n_h,n_y)\n",
        "b1=np.zeros((n_y))\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJODqTJ4AjHF"
      },
      "source": [
        "### Computing the cost\n",
        "Recall that for $m$ samples we defined the cross-entropy cost function as\n",
        "\\begin{align*}\n",
        "cost=\\frac{-1}{m}\\sum_s Y*\\log A^2+(1-Y)*\\log (1-A^2)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RYW0SaFGNH9"
      },
      "source": [
        "def loss(A2,Y):\n",
        "    m=Y.shape[0]\n",
        "    logprob=Y*np.log(A2)+(1-Y)*np.log(1-A2)\n",
        "    cost=-np.sum(logprob)/m\n",
        "    cost=np.squeeze(cost)\n",
        "    return cost"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjIUGuanAjGS"
      },
      "source": [
        "## Forward Propagation\n",
        "To implement forward propagation recall that \n",
        "  \\begin{align*}\n",
        "    Z^1&=W^0\\cdot A^0+b^0\\\\\n",
        "    A^1&=\\sigma(Z^1)\\\\\n",
        "    Z^2&=W^1\\cdot A^1+b^1\\\\\n",
        "    A^2&=\\sigma(Z^2)\n",
        "  \\end{align*}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-OaRqQFAjGU"
      },
      "source": [
        "def model(X):\n",
        "    Z1=np.dot(X,W0)+b0\n",
        "    A1=sigmoid(Z1)\n",
        "    Z2=np.dot(A1,W1)+b1\n",
        "    A2=sigmoid(Z2)\n",
        "    \n",
        "    return A1,A2"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq1IM6c7AjGi"
      },
      "source": [
        "## Back propagation\n",
        "To compute the gradients recall the formulas from class. Recall that $m$ is the number of samples, $n_x$ is the size of the input and $n_h$ is the width of the hidden layer. And $\\theta$ is the derivative of the sigmoid with respect to its argument. i.e. $\\sigma(1-\\sigma)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei1fUOXXAjGn"
      },
      "source": [
        "                                      Formula                                                   Shape\n",
        "\n",
        "\\begin{align*}\n",
        "   db^1&=\\frac{1}{m}\\sum_s(A^2-Y) & (1,1)\\\\\n",
        "      dW^1&=\\frac{1}{m}{A^1}^T\\cdot (A^2-Y) & (n_h,m)\\times (m,1)=(n_h,1)\\\\\n",
        "      db^0&=\\frac{1}{m}\\sum_s\\left[{W^1}^T\\cdot (A^2-Y)\\right]*\\theta & \\sum_s (n_h,1)\\times (1,m)=(n_h,1)\\\\\n",
        "      dW^0&=\\frac{1}{m}\\left[X^T\\cdot\\left({W^1}^T\\cdot (A^2-Y)\\right)*\\theta\\right] &(n_x,m)\\times(m,n_h)=(n_x,n_h)\n",
        "    \\end{align*}\n",
        "\n",
        "\n",
        "It is convenient to add temporary variables dZ2 and dZ1 defined as: $dZ2=A^2-Y$, $dZ1=\\left({W^1}^T\\cdot dZ2\\right)*\\sigma'$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyXDrFIvAjGq"
      },
      "source": [
        "def gradient(X,Y):\n",
        "    global dW0,db0,dW1,db1\n",
        "    #we will be dividing by the number of samples m\n",
        "    m=X.shape[0]\n",
        "    \n",
        "    A1,A2=model(X)\n",
        "    cost=loss(A2,Y)\n",
        "    \n",
        "    # the derivative of the sigmoid\n",
        "    gp=A1*(1-A1)\n",
        "    #we will use some temporary variables\n",
        "    dZ2=A2-Y\n",
        "    dW1=np.dot(A1.T,dZ2)/m\n",
        "    db1=np.sum(dZ2,axis=0,keepdims=True)/m\n",
        "    dZ1=np.dot(dZ2,W1.T)*gp\n",
        "    dW0=np.dot(X.T,dZ1)/m\n",
        "    db0=np.sum(dZ1,axis=0,keepdims=True)/m\n",
        "    return cost\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuMzsktaAjG5"
      },
      "source": [
        "### Updating the parameters\n",
        "For every iteration we need to update the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zQurzD7AjG6"
      },
      "source": [
        "def apply_gradients(learning_rate):\n",
        "\n",
        "    global W0,b0,W1,b1\n",
        "    W0=W0-learning_rate*dW0\n",
        "    b0=b0-learning_rate*db0\n",
        "    W1=W1-learning_rate*dW1\n",
        "    b1=b1-learning_rate*db1\n",
        "    "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofCF5L77AjHR"
      },
      "source": [
        "### Gradient Descent\n",
        "Having implemented all the above functions now we can implement gradient descent. Note that we are\n",
        "using the number of nodes in the hidden layer as a variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Ox-a2VAjHU",
        "tags": [],
        "outputId": "8c31f033-1aa4-4bad-872e-74290318ba99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(nb_iterations):\n",
        "    cost=gradient(x_train,y_train)\n",
        "    apply_gradients(learning_rate)\n",
        "    if i % 500 == 0:\n",
        "        print (\"Cost after iteration %i: %f\" %(i, cost))\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 4.210691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 500: 0.139088\n",
            "Cost after iteration 1000: 0.014069\n",
            "Cost after iteration 1500: 0.006289\n",
            "Cost after iteration 2000: 0.003874\n",
            "Cost after iteration 2500: 0.002745\n",
            "Cost after iteration 3000: 0.002102\n",
            "Cost after iteration 3500: 0.001690\n",
            "Cost after iteration 4000: 0.001407\n",
            "Cost after iteration 4500: 0.001200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iomBKV_LAjHe"
      },
      "source": [
        "### Evaluating the results\n",
        "At this point our network has learned the parameters. We test the predictions as follows: we compute the output $A^2$ and for every data point if the value of $A^2>0.5$ we predict Mine otherwise it is a Rock. After that we accumulate all the correct predictions. A prediction for data point $i$ is correct if $Y[i]=1$ and $A^2[i]=1$ or $Y[i]=0$ and \n",
        "$A^2[i]=0$. The sum of all correct predictions can be done nicely using the formula belwo\n",
        "\\begin{align*}\n",
        " {A^2}^T\\cdot Y+(1-{A^2}^T)\\cdot (1-Y)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "q8Na7ehlAjHq",
        "outputId": "536141a0-7718-42a2-d266-f652a286fd97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the output of both layers using the model\n",
        "A1,A2=model(x_test)\n",
        "#convert the predicted probabilities to Mine (1) or Rock(0) \n",
        "predictions=(A2>0.5)\n",
        "\n",
        "correct=np.dot(predictions.T,y_test)+np.dot(1-predictions.T,1-y_test)\n",
        "accuracy=100*float(correct)/float(y_test.shape[0])\n",
        "print(\"correct={} out of total of {}\".format(correct,y_test.shape[0]))\n",
        "print(\"Accuracy=\"+str(accuracy))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 1)\n",
            "(28, 1)\n",
            "correct=[[24.]] out of total of 28\n",
            "Accuracy=85.71428571428571\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}